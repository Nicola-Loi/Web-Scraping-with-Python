{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Storing Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Media Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can store a file by reference simply by storing the URL that the file is located in. This has several adavantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "imageLocation = bsObj.find(\"a\",{\"id\":\"logo\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve(imageLocation, \"logo.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download multiple internal files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = 'downloaded'\n",
    "baseUrl = \"http://pythonscraping.com\"\n",
    "\n",
    "def getAbsoluteURL(baseUrl,source):\n",
    "    if source.startswith(\"http://www.\"):\n",
    "        url = \"http://\"+source[11:]\n",
    "    elif source.stratswith(\"http://\"):\n",
    "        url = source\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = source[4:]\n",
    "        url = \"http://\"+source\n",
    "    else:\n",
    "        url = baseUrl+\"/\"+source\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace(\"www.\",\"\")\n",
    "    path = path.replace(baseUrl)\n",
    "    path = downloadDirectory+path\n",
    "    directory = os.path.dirname(path)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    return path\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "downloadList = bsObj.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download[\"src\"])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "\n",
    "urlretrieve(fileUrl,getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Storing Data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to modify a CSV file, or even creating one entirely from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvFile = open(\"../files/test.csv\",'w+')\n",
    "try:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(('number','number plus 2', 'number times 2'))\n",
    "    for i in range(10):\n",
    "        writer.writerow((i, i+2, i*2))\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common web-scraping task is to retrieve an HTML table and write it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://en.wikipedia.org/wiki/Comparison_of_text_editors\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "#The main comparison table is currently the first table on the page\n",
    "table = bsObj.findAll(\"table\",{\"class\":\"wikitable\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "csvFile = open(\"../files/editors.csv\",'wt')\n",
    "writer = csv.writer(csvFile)\n",
    "\n",
    "try:\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td','th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "            write.writerow(csvRow)\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrating mySQL with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our local MySQL server is running we'll be able to execute the following script succesfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', unix_socket='/imp/mysql.sock', user='root', passwd=None, db='mysql')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "cur.execute(\"SELECT * FROM pahes WHERE id=1\")\n",
    "print(cur.fetchone())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a code able to store data scraped from a HTML Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock', user='root', passwd=None, db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute(\"INSERT INTO pages (title, content) VALUES (\\\"%s\\\",\\\"%s\\\")\",(title,content))\n",
    "    cur.connection.commit()\n",
    "    \n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+articleUrl)\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    title = bsObj.find(\"h1\").find(\"span\").get_text()\n",
    "    content = bsObj.find(\"div\", {\"id\":\"mW-content-text\"}).find(\"p\").get_text()\n",
    "    store(title, content)\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "        print(newArticle)\n",
    "        links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
